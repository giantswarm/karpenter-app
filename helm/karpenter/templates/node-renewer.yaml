apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "karpenter.serviceAccountName" . }}-renewer
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "karpenter.labels" . | nindent 4 }}
  annotations:
  {{- if or .Values.karpenter.additionalAnnotations }}
  {{- with .Values.karpenter.additionalAnnotations }}
    {{- toYaml . | nindent 4 }}
  {{- end }}
  {{- end }}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ include "karpenter.serviceAccountName" . }}-renewer
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: {{ include "karpenter.serviceAccountName" . }}-renewer-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: {{ include "karpenter.serviceAccountName" . }}-renewer
subjects:
- kind: ServiceAccount
  name: {{ include "karpenter.serviceAccountName" . }}-renewer
  namespace: {{ .Release.Namespace }}
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: {{ include "karpenter.serviceAccountName" . }}-renewer
spec:
  schedule: "{{ .Values.renewerCronjob.schedule }}"
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: {{ include "karpenter.serviceAccountName" . }}-renewer
          restartPolicy: OnFailure
          containers:
          - name: renewer
            image: giantswarm/docker-kubectl:1.28.2
            command:
            - /bin/sh
            - -c
            - |
              #!/bin/sh

              echo "Starting node renewal process..."

              # Get the highest version of nodes without the "managed-by: karpenter" label
              echo "Fetching the highest version of nodes without the 'managed-by: karpenter' label..."
              TARGET_VERSION=$(kubectl get nodes -l 'managed-by notin (karpenter)' -o=jsonpath='{range .items[*]}{.metadata.labels.aws-operator\.giantswarm\.io/version}{"\n"}{end}' | sort -V | tail -n 1)

              if [ -z "$TARGET_VERSION" ]; then
                  echo "No target version found. Exiting."
                  exit 0
              fi

              echo "Target version identified as: $TARGET_VERSION"

              # Get the list of nodes with the "managed-by: karpenter" label and a different version
              echo "Fetching nodes with the 'managed-by: karpenter' label and a different version..."
              NODES_TO_DELETE=$(kubectl get nodes -l 'managed-by in (karpenter)' -o=jsonpath='{range .items[?(@.metadata.labels.aws-operator\.giantswarm\.io/version!="'$TARGET_VERSION'")]}{.metadata.name}{"\n"}{end}')

              # Check if NODES_TO_DELETE is empty
              if [ -z "$NODES_TO_DELETE" ]; then
                  echo "No nodes to delete. Exiting."
                  exit 0
              fi

              # Count the nodes
              TOTAL_NODES=$(echo "$NODES_TO_DELETE" | wc -l)

              echo "$TOTAL_NODES nodes identified with a different version."
              echo $NODES_TO_DELETE

              # Calculate 20% of the nodes
              NUM_NODES_TO_DELETE=$((TOTAL_NODES * {{ .Values.renewerCronjob.percentage }} / 100))

              # If there are nodes to delete but the calculated number is 0, delete at least one node
              if [ "$NUM_NODES_TO_DELETE" -eq 0 ] && [ "$TOTAL_NODES" -gt 0 ]; then
                  NUM_NODES_TO_DELETE=1
              fi

              echo "Planning to delete $NUM_NODES_TO_DELETE nodes."

              # Get the list of nodes to delete
              NODES_TO_DELETE=$(echo "$NODES_TO_DELETE" | head -n $NUM_NODES_TO_DELETE)

              # Delete the nodes
              for NODE in $NODES_TO_DELETE; do
                  echo "Deleting node: $NODE"
                  kubectl delete node $NODE --wait=false
              done

              echo "Node renewal process completed."  